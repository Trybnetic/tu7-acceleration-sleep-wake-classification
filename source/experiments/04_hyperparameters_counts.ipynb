{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fb9618",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Triaxial Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84825d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from utils import count_parameters\n",
    "from models import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Make code deterministic\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b05cbd",
   "metadata": {},
   "source": [
    "## Config and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762525b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "BASE_PATH = os.path.join(\"/home/source/experiments/\")\n",
    "RESULTS_BASE_PATH = os.path.join(BASE_PATH, \"results\")\n",
    "LOG_FILE_PATH = os.path.join(BASE_PATH, \"exp04_train.log\")\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'exp04_models')\n",
    "TRAIN_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, \"exp04_train.csv\")\n",
    "TEST_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, \"exp04_test.csv\")\n",
    "\n",
    "# Logging config\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename=LOG_FILE_PATH,\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Set information about the dataset\n",
    "HDF5_FILE_PATH = os.path.join(os.sep, 'home', 'data', \"ANNOTATED_BEDTIME_TU7.hdf5\")\n",
    "EPOCH_FILE_PATH = os.path.join(os.sep, 'home', 'data', \"ACTIGRAPH_EPOCHS_TU7_CLEAN.hdf5\")\n",
    "COLNAMES = [\"Time\", \"X\", \"Y\", \"Z\", \"Annotated Time in Bed\"]\n",
    "SAMPLE_RATE = 100\n",
    "LABEL_DICT = {False: 0, True: 1}\n",
    "EXCLUDED_DATASETS = []\n",
    "EPOCH_LENGTH = \"60s\"\n",
    "\n",
    "# Set information about the model, etc.\n",
    "INPUT_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "DROPOUT = 0.5 # https://jmlr.org/papers/v15/srivastava14a.html\n",
    "BATCH_SIZE = 8\n",
    "CLIP = 1000\n",
    "MAX_EPOCHS = 256\n",
    "MIN_EPOCHS = 0\n",
    "LR_DECAY = .9\n",
    "REVERSE = False\n",
    "\n",
    "# Combinations to test\n",
    "MODELS = [MLP, RNN, LSTM]\n",
    "HID_DIM = [1,2,4,8,16,32,64]\n",
    "N_LAYERS = [1,2,4]\n",
    "INIT_LR = [.7]\n",
    "\n",
    "# Minimal required loss impprovement\n",
    "EPSILON = 1e-4\n",
    "\n",
    "means = torch.Tensor([-0.0933,  0.3356, -0.2729])\n",
    "stds = torch.Tensor([0.4997, 0.4727, 0.5715])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294d76b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597cd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject(actigraph_file_path, epoch_file_path, subject, target_dict, colnames):\n",
    "    time_col, x_col, y_col, z_col, target_col = colnames\n",
    "\n",
    "    data = pd.read_hdf(actigraph_file_path, key=subject)\n",
    "    data = data[[time_col, target_col]]\n",
    "\n",
    "    epochs = pd.read_hdf(epoch_file_path, key=subject)\n",
    "\n",
    "    if min(data[time_col]).strftime('%Y-%d-%m') == min(epochs[time_col]).strftime('%Y-%m-%d'):\n",
    "        start_time = pd.Timestamp(min(data[time_col]).strftime('%Y-%d-%m %H:%M:%S')) # Apparently d and m are switched in the data.\n",
    "        data.loc[:,time_col] = pd.date_range(start_time, periods=data.shape[0], freq=\"10ms\")\n",
    "    else:\n",
    "        start_time = min(data[time_col])\n",
    "\n",
    "    end_time = max(data[time_col])\n",
    "\n",
    "    epochs = epochs.loc[(epochs[time_col] > start_time) & (epochs[time_col] < end_time), :]\n",
    "    epochs = epochs.set_index(time_col).resample(EPOCH_LENGTH).sum()\n",
    "\n",
    "    epochs = pd.merge_asof(epochs, data, on=time_col, direction=\"nearest\")\n",
    "    \n",
    "    #X = torch.from_numpy(np.array(epochs[y_col].values, dtype=np.float))\n",
    "    X = torch.from_numpy(np.array(epochs[[x_col, y_col, z_col]].values, dtype=np.float))\n",
    "    y = torch.from_numpy(epochs[target_col].apply(lambda label: target_dict[label]).values)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3daf363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61036532979f4c0e8f14f4c3876d2006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/445 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means = tensor([216.9997, 204.3047, 234.3755], dtype=torch.float64); stds = tensor([472.9218, 612.3429, 486.0117], dtype=torch.float64)\n",
      "Class 0 (awake): 0.54 +/- 0.11; Class 1 (sleep): 0.46 +/- 0.11\n",
      "Normalized the input of each channel (see train.py for details)\n",
      "Loaded 445 sequences with input shape [1665 x 3] and output shape [1665]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_subject_helper(subject): \n",
    "    return load_subject(HDF5_FILE_PATH, EPOCH_FILE_PATH, subject, LABEL_DICT, COLNAMES)\n",
    "\n",
    "\n",
    "def load_dataset(file_path, epoch_file_path, subjects, label_dict, colnames):\n",
    "    \n",
    "    with Pool(200) as p:\n",
    "        X, y = zip(*list(tqdm(p.imap(load_subject_helper, subjects), total=len(subjects))))\n",
    "\n",
    "    lengths = [elem.shape[0] for elem in X]\n",
    "\n",
    "    X, y, lengths = zip(*[(X[ii], y[ii], lengths[ii]) for ii in np.argsort(lengths)[::-1]])\n",
    "\n",
    "    means, stds = torch.cat(X).mean(axis=0), torch.cat(X).std(axis=0)\n",
    "\n",
    "    logging.info(f\"means = {means}; stds = {stds}\")\n",
    "    print(f\"means = {means}; stds = {stds}\")\n",
    "\n",
    "    class_0, class_1 = zip(*[((elem == 0).sum().numpy()/elem.shape[0], (elem == 1).sum().numpy()/elem.shape[0]) for elem in y])\n",
    "    logging.info(f\"Class 0 (awake): {np.mean(class_0):.2f} +/- {np.std(class_0):.2f}; Class 1 (sleep): {np.mean(class_1):.2f} +/- {np.std(class_1):.2f}\")\n",
    "    print(f\"Class 0 (awake): {np.mean(class_0):.2f} +/- {np.std(class_0):.2f}; Class 1 (sleep): {np.mean(class_1):.2f} +/- {np.std(class_1):.2f}\")\n",
    "\n",
    "    X, y, lengths = pad_sequence(X, batch_first=True), pad_sequence(y, batch_first=True), torch.Tensor(lengths)\n",
    "\n",
    "    X = (X - means) / stds\n",
    "    logging.info(\"Normalized the input of each channel (see train.py for details)\")\n",
    "    print(\"Normalized the input of each channel (see train.py for details)\")\n",
    "\n",
    "    return X, y, lengths\n",
    "\n",
    "# Select device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load available subjects\n",
    "with h5py.File(HDF5_FILE_PATH) as hdf5_file:\n",
    "    subjects = [subject for subject in hdf5_file.keys() if subject not in EXCLUDED_DATASETS]\n",
    "    \n",
    "# Load the data\n",
    "X, y, lengths = load_dataset(HDF5_FILE_PATH, EPOCH_FILE_PATH, subjects, LABEL_DICT, COLNAMES)\n",
    "X, y = X.float(), y.float()\n",
    "X, y, lengths = X.to(device), y.to(device), lengths.to(device)\n",
    "assert X.shape[0] == y.shape[0]\n",
    "print(f\"Loaded {X.shape[0]} sequences with input shape [{X.shape[1]} x {X.shape[2]}] and output shape [{y.shape[1]}]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8335e",
   "metadata": {},
   "source": [
    "## Create result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc67ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_RESULTS_PATH, \"w\") as f:\n",
    "    f.write(\"Combination,Fold,Epoch,Train Loss,Validation Loss,Hidden Dimension,Number of Layers,Initial Learning Rate,Model\\n\")\n",
    "logging.info(f\"Created training result file at {TRAIN_RESULTS_PATH}\")\n",
    "\n",
    "with open(TEST_RESULTS_PATH, \"w\") as f:\n",
    "    f.write(\"Combination,Fold,Loss,Accuracy,Precision,Recall,F1 Score,Hidden Dimension,Number of Layers,Initial Learning Rate,Model,Ellapsed Time\\n\")\n",
    "logging.info(f\"Created test result file at {TEST_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa251c8",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89faa228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a4ee13796348d9a5fa6d8fcfe07e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bc4a82a5350b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/source/experiments/models.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mseq_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[1;32m   2526\u001b[0m         input, target, weight, reduction_enum)\n\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "combinations = [(0, INIT_LR[0], 0, GLM)] + list(product(N_LAYERS, INIT_LR, HID_DIM, MODELS))\n",
    "\n",
    "n_combinations = len(combinations)\n",
    "for combination, (n_layers, init_lr, hid_dim, model_constr) in enumerate(tqdm(combinations)):\n",
    "\n",
    "    logging.info(f\"Combination {combination}: hid_dim = {hid_dim}; n_layers = {n_layers}; init_lr = {init_lr}; device = {device}\")\n",
    "\n",
    "    # Do 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(np.arange(X.size(0)))):\n",
    "\n",
    "        # Create validation data\n",
    "        train_idx, valid_idx = train_test_split(np.arange(train_idx.shape[0]), test_size=0.2)\n",
    "\n",
    "        # Create model and init weights\n",
    "        model = model_constr(INPUT_DIM, hid_dim, OUTPUT_DIM, n_layers, dropout=DROPOUT, batch_first=True)\n",
    "        logging.info('Model initialized with %s trainable parameters' % count_parameters(model))\n",
    "\n",
    "        # Init loss and optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=init_lr) # https://arxiv.org/abs/1409.3215\n",
    "        scheduler = ExponentialLR(optimizer, gamma=LR_DECAY)\n",
    "        criterion = nn.BCELoss()\n",
    "        logging.info(f\"Start with learning rate = {init_lr} (decay = {LR_DECAY}); batch size = {BATCH_SIZE}.\")\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(TensorDataset(X[train_idx], y[train_idx], lengths[train_idx]), batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(TensorDataset(X[valid_idx], y[valid_idx], lengths[valid_idx]), batch_size=BATCH_SIZE)\n",
    "        test_loader = DataLoader(TensorDataset(X[test_idx], y[test_idx], lengths[test_idx]), batch_size=BATCH_SIZE)\n",
    "        logging.info(f\"Use {len(train_idx)} sequences for training, {len(valid_idx)} sequences for validation and {len(test_idx)} sequences for testing.\")\n",
    "\n",
    "        # Set path and init best loss\n",
    "        best_model_path = os.path.join(MODEL_BASE_PATH, f'{combination:02d}_best_{n_layers}l_{model.name}{hid_dim}_model_fold_{fold}.pt')\n",
    "        best_valid_loss = float('inf')\n",
    "        epoch = 0\n",
    "\n",
    "        overall_start_time = time.time()\n",
    "\n",
    "        # Evaluate model without any training\n",
    "        train_loss, _ = evaluate(model, train_loader, criterion)\n",
    "        valid_loss, _ = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "        # Save losses to result file\n",
    "        with open(TRAIN_RESULTS_PATH, \"a\") as f:\n",
    "            f.write(f\"{combination},{fold},{epoch},{train_loss},{valid_loss},{hid_dim},{n_layers},{init_lr},{model.name}\\n\")\n",
    "\n",
    "        for epoch in range(1, MAX_EPOCHS + 1):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "            valid_loss, _ = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "            time_diff = int(time.time() - start_time)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if valid_loss + EPSILON < best_valid_loss:\n",
    "                # Save losses to result file\n",
    "                with open(TRAIN_RESULTS_PATH, \"a\") as f:\n",
    "                    f.write(f\"{combination},{fold},{epoch},{train_loss},{valid_loss},{hid_dim},{n_layers},{init_lr},{model.name}\\n\")\n",
    "\n",
    "                # Update best validation loss and save model\n",
    "                best_valid_loss = valid_loss\n",
    "                logging.info(f\"Updated best validation loss to {best_valid_loss}.\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                logging.info(f\"End training after epoch {epoch} as validation loss does not further decrease.\")\n",
    "                logging.info(f\"Best model saved at {best_model_path}\")\n",
    "                break\n",
    "\n",
    "        time_diff = int(time.time() - overall_start_time)\n",
    "\n",
    "        # Evaluate model on test set\n",
    "        logging.info(f\"Load model from epoch {epoch-1} from {best_model_path}\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        test_loss, metrics = evaluate(model, test_loader, criterion)\n",
    "        accuracy, precision, recall, f1_score = metrics\n",
    "\n",
    "        with open(TEST_RESULTS_PATH, \"a\") as f:\n",
    "            f.write(f\"{combination},{fold},{test_loss},{accuracy},{precision},{recall},{f1_score},{hid_dim},{n_layers},{init_lr},{model.name},{time_diff}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6bc3791c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = zip(*list(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b831d524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True),\n",
       " tensor(True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[torch.all((xx >= 0) & (xx <= 1)) for xx in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34521658",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "all elements of input should be between 0 and 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5d518875e6ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mseq_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-docs/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2523\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2525\u001b[0;31m     return torch._C._nn.binary_cross_entropy(\n\u001b[0m\u001b[1;32m   2526\u001b[0m         input, target, weight, reduction_enum)\n\u001b[1;32m   2527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: all elements of input should be between 0 and 1"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch_loss = 0\n",
    "\n",
    "for i, (X, y, lens) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    y_pred = model(X, lens)\n",
    "    loss = 0\n",
    "    for ii, ll in enumerate(lens):\n",
    "        seq_true, seq_pred = y[ii,:int(ll)].squeeze(), y_pred[ii,:int(ll)].squeeze()\n",
    "        loss += criterion(seq_pred, seq_true) / len(lens)\n",
    "\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "return epoch_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "31bb84d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan,  ..., nan, nan, nan], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f449042c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): RNN(3, 64, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
