{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91fb9618",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization (Triaxial Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e84825d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "from itertools import product\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "from utils import count_parameters\n",
    "from models import *\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Make code deterministic\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b05cbd",
   "metadata": {},
   "source": [
    "## Config and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762525b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file paths\n",
    "BASE_PATH = os.path.join(\"/home/source/experiments/\")\n",
    "RESULTS_BASE_PATH = os.path.join(BASE_PATH, \"results\")\n",
    "LOG_FILE_PATH = os.path.join(BASE_PATH, \"exp04_train.log\")\n",
    "MODEL_BASE_PATH = os.path.join(BASE_PATH, 'exp04_models')\n",
    "TRAIN_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, \"exp04_train.csv\")\n",
    "TEST_RESULTS_PATH = os.path.join(RESULTS_BASE_PATH, \"exp04_test.csv\")\n",
    "\n",
    "# Logging config\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename=LOG_FILE_PATH,\n",
    "                    format='%(asctime)s.%(msecs)03d %(levelname)s %(module)s - %(funcName)s: %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Set information about the dataset\n",
    "HDF5_FILE_PATH = os.path.join(os.sep, 'home', 'data', \"ANNOTATED_BEDTIME_TU7.hdf5\")\n",
    "EPOCH_FILE_PATH = os.path.join(os.sep, 'home', 'data', \"ACTIGRAPH_EPOCHS_TU7_CLEAN.hdf5\")\n",
    "COLNAMES = [\"Time\", \"X\", \"Y\", \"Z\", \"Annotated Time in Bed\"]\n",
    "SAMPLE_RATE = 100\n",
    "LABEL_DICT = {False: 0, True: 1}\n",
    "EXCLUDED_DATASETS = [\"subject90067325\"]\n",
    "EPOCH_LENGTH = \"60s\"\n",
    "\n",
    "# Set information about the model, etc.\n",
    "INPUT_DIM = 3\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "DROPOUT = 0.5 # https://jmlr.org/papers/v15/srivastava14a.html\n",
    "BATCH_SIZE = 8\n",
    "CLIP = 1000\n",
    "MAX_EPOCHS = 256\n",
    "MIN_EPOCHS = 0\n",
    "LR_DECAY = .9\n",
    "REVERSE = False\n",
    "\n",
    "# Combinations to test\n",
    "MODELS = [MLP, RNN, LSTM]\n",
    "HID_DIM = [1,2,4,8,16,32,64]\n",
    "N_LAYERS = [1,2,4]\n",
    "INIT_LR = [.7]\n",
    "\n",
    "# Minimal required loss impprovement\n",
    "EPSILON = 1e-4\n",
    "\n",
    "means = torch.Tensor([217.0340, 204.4965, 234.5470])\n",
    "stds = torch.Tensor([473.0284, 612.7618, 486.2913])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294d76b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "597cd484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subject(actigraph_file_path, epoch_file_path, subject, target_dict, colnames):\n",
    "    time_col, x_col, y_col, z_col, target_col = colnames\n",
    "\n",
    "    data = pd.read_hdf(actigraph_file_path, key=subject)\n",
    "    data = data[[time_col, target_col]]\n",
    "\n",
    "    epochs = pd.read_hdf(epoch_file_path, key=subject)\n",
    "\n",
    "    if min(data[time_col]).strftime('%Y-%d-%m') == min(epochs[time_col]).strftime('%Y-%m-%d'):\n",
    "        start_time = pd.Timestamp(min(data[time_col]).strftime('%Y-%d-%m %H:%M:%S')) # Apparently d and m are switched in the data.\n",
    "        data.loc[:,time_col] = pd.date_range(start_time, periods=data.shape[0], freq=\"10ms\")\n",
    "    else:\n",
    "        start_time = min(data[time_col])\n",
    "\n",
    "    end_time = max(data[time_col])\n",
    "\n",
    "    epochs = epochs.loc[(epochs[time_col] > start_time) & (epochs[time_col] < end_time), :]\n",
    "    epochs = epochs.set_index(time_col).resample(EPOCH_LENGTH).sum()\n",
    "\n",
    "    epochs = pd.merge_asof(epochs, data, on=time_col, direction=\"nearest\")\n",
    "    \n",
    "    #X = torch.from_numpy(np.array(epochs[y_col].values, dtype=np.float))\n",
    "    X = torch.from_numpy(np.array(epochs[[x_col, y_col, z_col]].values, dtype=np.float))\n",
    "    y = torch.from_numpy(epochs[target_col].apply(lambda label: target_dict[label]).values)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3daf363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee384a92714f43a2078398ec297e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/444 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "means = tensor([217.0340, 204.4965, 234.5470], dtype=torch.float64); stds = tensor([473.0284, 612.7618, 486.2913], dtype=torch.float64)\n",
      "Class 0 (awake): 0.54 +/- 0.11; Class 1 (sleep): 0.46 +/- 0.11\n",
      "Normalized the input of each channel (see train.py for details)\n",
      "Loaded 444 sequences with input shape [1665 x 3] and output shape [1665]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def load_subject_helper(subject): \n",
    "    return load_subject(HDF5_FILE_PATH, EPOCH_FILE_PATH, subject, LABEL_DICT, COLNAMES)\n",
    "\n",
    "\n",
    "def load_dataset(file_path, epoch_file_path, subjects, label_dict, colnames):\n",
    "    \n",
    "    with Pool(200) as p:\n",
    "        X, y = zip(*list(tqdm(p.imap(load_subject_helper, subjects), total=len(subjects))))\n",
    "\n",
    "    lengths = [elem.shape[0] for elem in X]\n",
    "\n",
    "    X, y, lengths = zip(*[(X[ii], y[ii], lengths[ii]) for ii in np.argsort(lengths)[::-1]])\n",
    "\n",
    "    means, stds = torch.cat(X).mean(axis=0), torch.cat(X).std(axis=0)\n",
    "\n",
    "    logging.info(f\"means = {means}; stds = {stds}\")\n",
    "    print(f\"means = {means}; stds = {stds}\")\n",
    "\n",
    "    class_0, class_1 = zip(*[((elem == 0).sum().numpy()/elem.shape[0], (elem == 1).sum().numpy()/elem.shape[0]) for elem in y])\n",
    "    logging.info(f\"Class 0 (awake): {np.mean(class_0):.2f} +/- {np.std(class_0):.2f}; Class 1 (sleep): {np.mean(class_1):.2f} +/- {np.std(class_1):.2f}\")\n",
    "    print(f\"Class 0 (awake): {np.mean(class_0):.2f} +/- {np.std(class_0):.2f}; Class 1 (sleep): {np.mean(class_1):.2f} +/- {np.std(class_1):.2f}\")\n",
    "\n",
    "    X, y, lengths = pad_sequence(X, batch_first=True), pad_sequence(y, batch_first=True), torch.Tensor(lengths)\n",
    "\n",
    "    X = (X - means) / stds\n",
    "    logging.info(\"Normalized the input of each channel (see train.py for details)\")\n",
    "    print(\"Normalized the input of each channel (see train.py for details)\")\n",
    "\n",
    "    return X, y, lengths\n",
    "\n",
    "# Select device (GPU if available)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load available subjects\n",
    "with h5py.File(HDF5_FILE_PATH) as hdf5_file:\n",
    "    subjects = [subject for subject in hdf5_file.keys() if subject not in EXCLUDED_DATASETS]\n",
    "    \n",
    "# Load the data\n",
    "X, y, lengths = load_dataset(HDF5_FILE_PATH, EPOCH_FILE_PATH, subjects, LABEL_DICT, COLNAMES)\n",
    "X, y = X.float(), y.float()\n",
    "X, y, lengths = X.to(device), y.to(device), lengths.to(device)\n",
    "assert X.shape[0] == y.shape[0]\n",
    "print(f\"Loaded {X.shape[0]} sequences with input shape [{X.shape[1]} x {X.shape[2]}] and output shape [{y.shape[1]}]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca8335e",
   "metadata": {},
   "source": [
    "## Create result files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc67ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAIN_RESULTS_PATH, \"w\") as f:\n",
    "    f.write(\"Combination,Fold,Epoch,Train Loss,Validation Loss,Hidden Dimension,Number of Layers,Initial Learning Rate,Model\\n\")\n",
    "logging.info(f\"Created training result file at {TRAIN_RESULTS_PATH}\")\n",
    "\n",
    "with open(TEST_RESULTS_PATH, \"w\") as f:\n",
    "    f.write(\"Combination,Fold,Loss,Accuracy,Precision,Recall,F1 Score,Hidden Dimension,Number of Layers,Initial Learning Rate,Model,Ellapsed Time\\n\")\n",
    "logging.info(f\"Created test result file at {TEST_RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa251c8",
   "metadata": {},
   "source": [
    "## Train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89faa228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242fba3e29f2410d9ae644c519e0b6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combinations = [(0, INIT_LR[0], 0, GLM)] + list(product(N_LAYERS, INIT_LR, HID_DIM, MODELS))\n",
    "#combinations = combinations[17:]\n",
    "\n",
    "n_combinations = len(combinations)\n",
    "for combination, (n_layers, init_lr, hid_dim, model_constr) in enumerate(tqdm(combinations)):\n",
    "\n",
    "    logging.info(f\"Combination {combination}: hid_dim = {hid_dim}; n_layers = {n_layers}; init_lr = {init_lr}; device = {device}\")\n",
    "\n",
    "    # Do 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10)\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(np.arange(X.size(0)))):\n",
    "\n",
    "        # Create validation data\n",
    "        train_idx, valid_idx = train_test_split(np.arange(train_idx.shape[0]), test_size=0.2)\n",
    "\n",
    "        # Create model and init weights\n",
    "        model = model_constr(INPUT_DIM, hid_dim, OUTPUT_DIM, n_layers, dropout=DROPOUT, batch_first=True)\n",
    "        logging.info('Model initialized with %s trainable parameters' % count_parameters(model))\n",
    "\n",
    "        # Init loss and optimizer\n",
    "        optimizer = optim.SGD(model.parameters(), lr=init_lr) # https://arxiv.org/abs/1409.3215\n",
    "        scheduler = ExponentialLR(optimizer, gamma=LR_DECAY)\n",
    "        criterion = nn.BCELoss()\n",
    "        logging.info(f\"Start with learning rate = {init_lr} (decay = {LR_DECAY}); batch size = {BATCH_SIZE}.\")\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(TensorDataset(X[train_idx], y[train_idx], lengths[train_idx]), batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(TensorDataset(X[valid_idx], y[valid_idx], lengths[valid_idx]), batch_size=BATCH_SIZE)\n",
    "        test_loader = DataLoader(TensorDataset(X[test_idx], y[test_idx], lengths[test_idx]), batch_size=BATCH_SIZE)\n",
    "        logging.info(f\"Use {len(train_idx)} sequences for training, {len(valid_idx)} sequences for validation and {len(test_idx)} sequences for testing.\")\n",
    "\n",
    "        # Set path and init best loss\n",
    "        best_model_path = os.path.join(MODEL_BASE_PATH, f'{combination:02d}_best_{n_layers}l_{model.name}{hid_dim}_model_fold_{fold}.pt')\n",
    "        best_valid_loss = float('inf')\n",
    "        epoch = 0\n",
    "\n",
    "        overall_start_time = time.time()\n",
    "\n",
    "        # Evaluate model without any training\n",
    "        train_loss, _ = evaluate(model, train_loader, criterion)\n",
    "        valid_loss, _ = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "        # Save losses to result file\n",
    "        with open(TRAIN_RESULTS_PATH, \"a\") as f:\n",
    "            f.write(f\"{combination},{fold},{epoch},{train_loss},{valid_loss},{hid_dim},{n_layers},{init_lr},{model.name}\\n\")\n",
    "\n",
    "        for epoch in range(1, MAX_EPOCHS + 1):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n",
    "            valid_loss, _ = evaluate(model, valid_loader, criterion)\n",
    "\n",
    "            time_diff = int(time.time() - start_time)\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "            if valid_loss + EPSILON < best_valid_loss:\n",
    "                # Save losses to result file\n",
    "                with open(TRAIN_RESULTS_PATH, \"a\") as f:\n",
    "                    f.write(f\"{combination},{fold},{epoch},{train_loss},{valid_loss},{hid_dim},{n_layers},{init_lr},{model.name}\\n\")\n",
    "\n",
    "                # Update best validation loss and save model\n",
    "                best_valid_loss = valid_loss\n",
    "                logging.info(f\"Updated best validation loss to {best_valid_loss}.\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                logging.info(f\"End training after epoch {epoch} as validation loss does not further decrease.\")\n",
    "                logging.info(f\"Best model saved at {best_model_path}\")\n",
    "                break\n",
    "\n",
    "        time_diff = int(time.time() - overall_start_time)\n",
    "\n",
    "        # Evaluate model on test set\n",
    "        logging.info(f\"Load model from epoch {epoch-1} from {best_model_path}\")\n",
    "        model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "        test_loss, metrics = evaluate(model, test_loader, criterion)\n",
    "        accuracy, precision, recall, f1_score = metrics\n",
    "\n",
    "        with open(TEST_RESULTS_PATH, \"a\") as f:\n",
    "            f.write(f\"{combination},{fold},{test_loss},{accuracy},{precision},{recall},{f1_score},{hid_dim},{n_layers},{init_lr},{model.name},{time_diff}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
